{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mlflow\n",
    "import tensorflow as tf\n",
    "from sklearn import model_selection, metrics\n",
    "from datetime import datetime\n",
    "\n",
    "from src_nowcasting import image_preprocessing, sequence_img_generator, get_models, constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing\n",
    "\n",
    "- preparation of the dataframe with ***'Target', 'Irr', 'Image'*** columns\n",
    "- preprocessing and saving of the acquired images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processor = image_preprocessing.PreProcessImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all the days in the folder\n",
    "for day in os.listdir(constants.PATH_INPUT_FOLDER):    \n",
    "\n",
    "   # go through all the images in the day\n",
    "    files = os.listdir(os.path.join(constants.PATH_INPUT_FOLDER, day))\n",
    "\n",
    "    for f in files:\n",
    "\n",
    "        # Load image\n",
    "        in_path = os.path.join(constants.PATH_INPUT_FOLDER, day, f)\n",
    "        image =  cv2.imread(in_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        # Transform image\n",
    "        new_image = pre_processor.transform_image(image)\n",
    "\n",
    "        # Save image\n",
    "        folder_path = os.path.join(constants.PATH_OUTPUT_FOLDER, day)\n",
    "        # Check if exists and if not create folder\n",
    "        if not os.path.exists(folder_path): os.mkdir(folder_path)\n",
    "        out_path = os.path.join(folder_path, f.split('.')[0]+'.jpg')\n",
    "\n",
    "        cv2.imwrite(out_path, new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = sequence_img_generator.generate_dataframe(constants.PATH_WEATHER_FILES, FORECAST_HORIZON = 15)\n",
    "df_data.to_parquet(r'..\\dataset\\df_data.parquet.gzip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize keras session (ML library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize mlflow session (experiment tracking library):\n",
    "\n",
    "- parts related to mlflow can be deleted (if checkpointing is active)\n",
    "- in the provided code only single model with best parameters will be run (trivial experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///c:/Users/Admin/Code/maciej-solar-nowcasting/nowcasting/mlruns/4', creation_time=1696335798033, experiment_id='4', last_update_time=1696335798033, lifecycle_stage='active', name='sCNN_best_model_tests', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"sCNN_best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Target column according to needs\n",
    "df_data['Target'] = df_data.Target_CSI\n",
    "\n",
    "# Remove the data with low elevation\n",
    "df_data_reduced = df_data[df_data.elevation > constants.ELEVATION_THRESHOLD]\n",
    "\n",
    "\n",
    "df_train_full, df_test = model_selection.train_test_split(df_data_reduced, train_size=constants.TRAIN_SIZE, shuffle=False)\n",
    "df_train, df_val = model_selection.train_test_split(df_train_full, train_size=constants.TRAIN_SIZE, shuffle=False)\n",
    "\n",
    "train_generator = sequence_img_generator.DataGeneratorGHI_SCNN(df_train, constants.PATH_OUTPUT_FOLDER, **constants.train_params)\n",
    "\n",
    "val_generator = sequence_img_generator.DataGeneratorGHI_SCNN(df_val, constants.PATH_OUTPUT_FOLDER, **constants.valid_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test cases: \n",
    "\n",
    "- Sunny day test - 19/08/2023, 23/08/2023\n",
    "- Partially cloudy day test - 26/08/2023, 29/08/2023\n",
    " - Mostly cloudy / rainy day test - 27/08/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_1 = df_test[df_test.date.dt.date == datetime(2023, 8, 19).date()].copy() # Sunny day\n",
    "df_test_2 = df_test[df_test.date.dt.date == datetime(2023, 8, 23).date()].copy() # Sunny day\n",
    "df_test_3 = df_test[df_test.date.dt.date == datetime(2023, 8, 26).date()].copy() # Partially cloudy day\n",
    "df_test_4 = df_test[df_test.date.dt.date == datetime(2023, 8, 27).date()].copy() # Mostly cloudy / rainy day\n",
    "df_test_5 = df_test[df_test.date.dt.date == datetime(2023, 8, 29).date()].copy() # Partially cloudy day\n",
    "\n",
    "test_cases = [df_test_1, df_test_2, df_test_3, df_test_4, df_test_5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_models.SCNN_small(input_shape=[constants.IMG_SIZE[0], constants.IMG_SIZE[1], constants.NO_IMAGES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.tensorflow.autolog(disable=True)\n",
    "\n",
    "# Model parameters\n",
    "BETA_1 = 0.9\n",
    "BETA_2 = 0.999\n",
    "LEARNING_RATE_START = 0.0003\n",
    "LOSS = 'mean_squared_error'\n",
    "\n",
    "RUN_ID = 1\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=f'run_{RUN_ID:03d}_{constants.MODEL_TYPE}_scaled_15, lr: {LEARNING_RATE_START}, loss: {LOSS}'):\n",
    "    \n",
    "    params = {\n",
    "        'forecast_horizon': constants.FORECAST_HORIZON,\n",
    "        'elevation_threshold': constants.ELEVATION_THRESHOLD,\n",
    "        'model_type': constants.MODEL_TYPE,\n",
    "        'learning_rate': LEARNING_RATE_START,\n",
    "        'beta_1': BETA_1,\n",
    "        'beta_2': BETA_2,\n",
    "        'loss': 'mean_squared_error',    \n",
    "    }\n",
    "\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    callbacks_list = []\n",
    "    \n",
    "    # Logging\n",
    "    if constants.LOG_PATH: \n",
    "        callbacks_list.append(tf.keras.callbacks.CSVLogger(os.path.join(constants.LOG_PATH, f'training_id_{constants.MODEL_TYPE}_{LEARNING_RATE_START}.csv')))\n",
    "    # Checkpointing\n",
    "    if constants.CHECKPOINT_PATH:\n",
    "        callbacks_list.append(tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(constants.CHECKPOINT_PATH, f'training_id_{constants.MODEL_TYPE}_scaled_{constants.FORECAST_HORIZON}_{LEARNING_RATE_START}_{LOSS}.h5'),\n",
    "            verbose = 1,\n",
    "            save_best_only = True,\n",
    "            ))\n",
    "\n",
    "    # Early stopping\n",
    "    callbacks_list.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10))\n",
    "    # Learing rate reduction scheduler\n",
    "    # callbacks_list.append(get_models.OneCycleScheduler(math.ceil(len(df_train) / train_batchsize) * epochs, max_rate = 0.0005))\n",
    "    # callbacks_list.append(tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn))\n",
    "                \n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=LEARNING_RATE_START, \n",
    "        beta_1=0.9, \n",
    "        beta_2=0.999, \n",
    "        amsgrad=False\n",
    "        )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer, \n",
    "        loss=LOSS,\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "        )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        #steps_per_epoch=int(df_train.shape[0] / train_batchsize),\n",
    "        epochs=constants.EPOCHS,\n",
    "        validation_data=val_generator,\n",
    "        #validation_steps=int(df_val.shape[0] / train_batchsize),\n",
    "        callbacks=callbacks_list                              \n",
    "        )\n",
    "        \n",
    "    mlflow.log_param(\"model_params\", model.count_params())\n",
    "\n",
    "            \n",
    "    for i_test, df_t in enumerate(test_cases):\n",
    "            \n",
    "        test_generator = sequence_img_generator.DataGeneratorGHI_SCNN(df_t, constants.PATH_OUTPUT_FOLDER, **constants.test_params)\n",
    "\n",
    "        # Test ghi\n",
    "        y_test = model.predict(test_generator) * df_t.Target_GHICS.values.reshape(-1, 1)\n",
    "        y_true = df_t.Target_GHIr.values\n",
    "        y_pers = df_t.ghi1.values\n",
    "                \n",
    "                \n",
    "        mae_test = metrics.mean_squared_error(y_true, y_test)\n",
    "        mae_per = metrics.mean_squared_error(y_true, y_pers)\n",
    "                \n",
    "        FS = 1 - mae_test / mae_per\n",
    "            \n",
    "        print(f'Test case: {df_t.date.dt.date.iloc[0]}')    \n",
    "        print(f\"model_params: {model.count_params()}\")\n",
    "        print(f\"mae_test: {mae_test}\")\n",
    "        print(f\"mae_pers {mae_per}\")\n",
    "        print(f\"FS: {FS}\")\n",
    "            \n",
    "        mlflow.log_metric(f'mae_test_{i_test}', mae_test)\n",
    "        mlflow.log_metric(f\"mae_pers_{i_test}\", mae_per)\n",
    "        mlflow.log_metric(f\"FS_{i_test}\", FS)\n",
    "            \n",
    "            \n",
    "    mlflow.tensorflow.log_model(model, f'{constants.MODEL_TYPE}_{LEARNING_RATE_START}_{LOSS}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A2A",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7dcf80bfe24a4c1c599ee178e16a27a6ca789ca5e1840aa335d10d61a7af269"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
